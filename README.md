# Passthrough HandsFreeController

A Mixed Reality Interface for Robotic Wheelchair Navigation Control, developed as an undergraduate thesis project for the Advanced Robotics Laboratory (AdRoLab) at FEEC/UNICAMP.

## Overview

This project is a Unity application designed for the Meta Quest 3 headset, serving as a hands-free Human-Machine Interface (HMI) for controlling a robotic wheelchair. The primary goal is to provide an intuitive navigation system for users with motor disabilities.

The system leverages the mixed reality (MR) capabilities of the Quest 3, allowing the user to see their real environment (via passthrough) and select a target destination on the floor simply by using their head movements. Once a target is selected and confirmed, the application autonomously commands the wheelchair to rotate and navigate to the chosen point.

## Core Technologies

- **Hardware**: Meta Quest 3, AdRoLab Robotic Wheelchair.

- **Software**: Unity (v. 6000.0.44f1), C#.

- **SDKs & Kits**:

  - **Meta XR All-in-One SDK**: Provides the core libraries for rendering, tracking, and input/output on the Meta Horizon OS.

  - **Mixed Reality Utility Kit**: Used to import and interact with the environment's semantic meshes (e.g., floor, walls) generated by the headset's room setup.

- **Communication**: The application sends navigation commands to the wheelchair's central server via HTTP requests over Wi-Fi, using the exposed RestThru API.

## Project Structure

The application is built within a single Unity scene following Unity's standard Component-Based Architecture. The system's logic is encapsulated in three primary custom modules (GameObjects):

1. **ApplicationController**

- **Role**: The central orchestrator of the entire system.
- **Implementation**: It is implemented as a Finite State Machine (FSM).
- **Function**: Manages the application's state (e.g., Main Menu, Target Selection, Moving, Rotating) and controls the activation/deactivation of the other modules. It consumes the interfaces provided by CameraGazeCursor and MovementSequenceController to execute the application flow.

2. **CameraGazeCursor**

- **Role**: Encapsulates all logic for user interaction and target selection via head movement.
- **Function**:
  - When active, it uses raycasting from the headset's CenterEyeAnchor (provided by the Camera Rig prefab).
  - It performs semantic matching to detect when the user's gaze intersects with the environment's floor mesh (provided by the MR Utility Kit).
  - It renders a cursor at the collision point on the floor.
  - A dwell-time (hover) logic is used to initiate a confirmation UI.
  - When the user confirms a target, this module fires an event, providing the 3D coordinates of the selected point to the ApplicationController.

3. **MovementSequenceController**

- **Role**: Implements the complete navigation logic and communication with the wheelchair.
- **Function**:
  - Tracks the wheelchair's real-time position and orientation using an AnchorManager component.
  - Provides two main movement primitives that are called by the ApplicationController:
  - PerformLinearDisplacement: A "point-to-go" function that autonomously moves the wheelchair to the target coordinates received from the CameraGazeCursor.
  - FollowTargetAligment: A rotation-only mode where the wheelchair turns on its axis to match the user's head (headset) orientation.

- **Sub-components**:
  -  **RestThruAPI**: This class acts as an abstraction layer for the wheelchair's HTTP server. It encapsulates the navigation logic into high-level asynchronous operations, such as GotoPoint2D (for linear movement) and AlignHeading2D (for rotation).

## Core Functionality Flow

1. The ApplicationController starts in the MainMenuState.
2. The user selects the "Move" option (using gaze-based UI interaction), transitioning the FSM to the SelectTargetState.
3. The ApplicationController activates the CameraGazeCursor module.
4. The user looks at a point on the floor, and the CameraGazeCursor detects it, placing a cursor.
5. After a brief dwell time, a UI panel (SelectTargetUI) appears, asking the user to "Confirm" or "Cancel" the movement.
6. The user gazes at "Confirm." The CameraGazeCursor fires the OnSelectionFinished event with the target's 3D coordinates and deactivates itself.
7. The ApplicationController catches this event, transitions to the MovingState, and calls PerformLinearDisplacement on the MovementSequenceController, passing the target point.
8. The MovementSequenceController and its RestThruAPI handle the autonomous navigation (initial alignment, then linear movement) to the target.
9. Once the movement is complete, the MovementSequenceController notifies the ApplicationController, which can then transition to another state (e.g., RotatingState or back to the main menu).
